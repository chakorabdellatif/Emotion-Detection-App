{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5a0483d3-865d-4a1a-87f4-2fc57edd9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f27fd-833a-4686-a9d8-6316f4a9a046",
   "metadata": {},
   "source": [
    "# Load VGG16 pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c46e7efc-e917-4204-9273-80b91236c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet',\n",
    "                   include_top=False, \n",
    "                   input_shape=(96, 96, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b39d7-064e-45a4-9c95-4ce5e4103970",
   "metadata": {},
   "source": [
    "# Freeze the layers of VGG16 (execpt the last 4 layer \"fine-tune\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bfcbb6fe-08cd-491b-aafb-591ad71b0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8636a-f775-4e9e-952d-c527b3c5d448",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4c1f5d18-7a96-4646-964b-782dce28c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(6, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f68c7baf-6b66-4f11-84f3-ba3fd21baec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m1,542\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,848,582</span> (56.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,848,582\u001b[0m (56.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,768,646</span> (29.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,768,646\u001b[0m (29.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,079,936</span> (27.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,079,936\u001b[0m (27.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88730730-5ccf-40d7-acc9-53d63405faad",
   "metadata": {},
   "source": [
    "# Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2cb4053d-b45e-4f0b-82b7-402f25e0b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate=0.0001), \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee792f-f73e-4463-a8ba-f56d6ef2d9b2",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "dcce4747-91ce-4085-96c4-ff251dbfb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,  \n",
    "    width_shift_range=0.3,  \n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "# validation data (without augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8407db0-0a80-4866-b19a-b7de0c8a3bae",
   "metadata": {},
   "source": [
    "# Load training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c0d02d94-2580-4359-9eb4-826e3813bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18192 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '../Emotion_application/train',\n",
    "    target_size = (96,96),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2c01b960-a30d-4ddd-92fc-3d75a4790cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6030 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    '../Emotion_application/test',\n",
    "    target_size = (96,96),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a606c12b-2431-440e-b8b5-4a2826f7494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for better training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2335ea-034f-4456-9b4b-8fd893dc4a62",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6aa81fbc-63cd-4267-8253-24cfcf69afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1078s\u001b[0m 2s/step - accuracy: 0.2438 - loss: 1.7932 - val_accuracy: 0.3333 - val_loss: 1.5784\n",
      "Epoch 2/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 615us/step - accuracy: 0.4062 - loss: 1.3071 - val_accuracy: 0.2857 - val_loss: 1.5188\n",
      "Epoch 3/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1076s\u001b[0m 2s/step - accuracy: 0.4613 - loss: 1.3047 - val_accuracy: 0.3908 - val_loss: 1.8124\n",
      "Epoch 4/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503us/step - accuracy: 0.6875 - loss: 0.7622 - val_accuracy: 0.2857 - val_loss: 2.7984\n",
      "Epoch 5/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1108s\u001b[0m 2s/step - accuracy: 0.5817 - loss: 1.0500 - val_accuracy: 0.6272 - val_loss: 0.9371\n",
      "Epoch 6/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 559us/step - accuracy: 0.7500 - loss: 0.7810 - val_accuracy: 0.7143 - val_loss: 0.6347\n",
      "Epoch 7/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1075s\u001b[0m 2s/step - accuracy: 0.6342 - loss: 0.9373 - val_accuracy: 0.5934 - val_loss: 1.1094\n",
      "Epoch 8/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 529us/step - accuracy: 0.6875 - loss: 0.7841 - val_accuracy: 0.6429 - val_loss: 1.1867\n",
      "Epoch 9/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1074s\u001b[0m 2s/step - accuracy: 0.6671 - loss: 0.8565 - val_accuracy: 0.4865 - val_loss: 1.5070\n",
      "Epoch 10/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 540us/step - accuracy: 0.5312 - loss: 0.9973 - val_accuracy: 0.5714 - val_loss: 1.3206\n",
      "Epoch 11/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 2s/step - accuracy: 0.6778 - loss: 0.8318 - val_accuracy: 0.6900 - val_loss: 0.8143\n",
      "Epoch 12/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504us/step - accuracy: 0.8438 - loss: 0.5680 - val_accuracy: 0.7857 - val_loss: 0.6992\n",
      "Epoch 13/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1074s\u001b[0m 2s/step - accuracy: 0.6940 - loss: 0.7834 - val_accuracy: 0.6689 - val_loss: 0.9024\n",
      "Epoch 14/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502us/step - accuracy: 0.6875 - loss: 0.7634 - val_accuracy: 0.6429 - val_loss: 0.9637\n",
      "Epoch 15/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1072s\u001b[0m 2s/step - accuracy: 0.7212 - loss: 0.7367 - val_accuracy: 0.6897 - val_loss: 0.7904\n",
      "Epoch 16/20\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 584us/step - accuracy: 0.6562 - loss: 0.7337 - val_accuracy: 0.8571 - val_loss: 0.6355\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs = 20,  \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8e1051c0-2800-497c-b925-8c05e9e7a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emotion_detection_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "8828f39f-77e8-45c4-aeaa-10639541f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\CHAKOR\\AppData\\Local\\Temp\\tmp6qozc88l\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\CHAKOR\\AppData\\Local\\Temp\\tmp6qozc88l\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\CHAKOR\\AppData\\Local\\Temp\\tmp6qozc88l'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name='input_layer_16')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1607003289360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607003289552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607003288016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607003288784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607003283792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1606967033488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607003284944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607003288976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007273424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007274576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007274768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007275344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007275536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007276112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007276304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007276880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007277072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007277648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007277840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007278416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007278608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007279184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007279376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007279952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007280144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007280720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007281680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007282064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007282640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007282832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007281488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007282256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007281104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1607007285136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model successfully converted to TensorFlow Lite!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model in .keras or .h5 format\n",
    "model = tf.keras.models.load_model('emotion_detection_model.keras')  # Or 'emotion_detection_model.h5'\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Optional: Enable optimizations\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open('emotion_detection_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model successfully converted to TensorFlow Lite!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e3788c1d-fae4-4d50-9f07-cc4e89261672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 510ms/step - accuracy: 0.6083 - loss: 1.0123\n",
      "Loss: 1.0005114078521729, Accuracy: 60.86%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy *100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
